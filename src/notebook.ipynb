{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd4d27",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing Audio Data\n",
    "\n",
    "Preprocessing involves:\n",
    "1. Loading and segmenting audio files\n",
    "2. Extracting content features (ContentVec/HuBERT)\n",
    "3. Extracting pitch features (F0)\n",
    "4. Building FAISS index for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb26a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.preprocessing import preprocess_for_training\n",
    "\n",
    "data_dir = \"../data/my_voice\"\n",
    "out_dir = \"../data/preprocessed\"\n",
    "\n",
    "preprocess_for_training(\n",
    "    data_dir=data_dir,\n",
    "    out_dir=out_dir,\n",
    "    quiet=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d49ab1",
   "metadata": {},
   "source": [
    "## Step 2: Training the Generator\n",
    "\n",
    "Train the voice conversion model on preprocessed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a473d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.training import train_generator_from_features\n",
    "\n",
    "feature_dir = \"../data/preprocessed\"\n",
    "model_name = \"my_voice_model\"\n",
    "\n",
    "generator = train_generator_from_features(\n",
    "    feature_dir=feature_dir,\n",
    "    epochs=100,\n",
    "    batch_size=4,\n",
    "    content_dim=768,\n",
    "    use_pitch=True,\n",
    "    target_sr=48000,\n",
    "    learning_rate=1e-4,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad8e94",
   "metadata": {},
   "source": [
    "## Step 3: Running Inference\n",
    "\n",
    "Convert a source audio file to the target speaker's voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc10a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.inference import run_inference\n",
    "from src.models.generator import Generator\n",
    "\n",
    "input_audio = \"../data/references/california_gurls.mp3\"\n",
    "output_audio = \"../data/output/converted_california_gurls.wav\"\n",
    "model_name = \"my_voice_model\"\n",
    "\n",
    "generator = Generator.load(model_name)\n",
    "\n",
    "run_inference(\n",
    "    input_path=input_audio,\n",
    "    generator=generator,\n",
    "    output_path=output_audio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e79d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "\n",
    "original_path = \"../data/references/california_gurls.mp3\"\n",
    "original_waveform, original_sr = torchaudio.load(original_path)\n",
    "\n",
    "print(\"Original Audio:\")\n",
    "ipd.display(ipd.Audio(original_waveform.numpy(), rate=original_sr))\n",
    "\n",
    "converted_path = \"../data/output/converted_california_gurls.wav\"\n",
    "try:\n",
    "    converted_waveform, converted_sr = torchaudio.load(converted_path)\n",
    "    print(\"\\nConverted Audio:\")\n",
    "    ipd.display(ipd.Audio(converted_waveform.numpy(), rate=converted_sr))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].plot(original_waveform[0].numpy()[:48000])\n",
    "    axes[0].set_title(\"Original Audio (first 1 second)\")\n",
    "    axes[0].set_xlabel(\"Sample\")\n",
    "    axes[0].set_ylabel(\"Amplitude\")\n",
    "    \n",
    "    axes[1].plot(converted_waveform[0].numpy()[:48000])\n",
    "    axes[1].set_title(\"Converted Audio (first 1 second)\")\n",
    "    axes[1].set_xlabel(\"Sample\")\n",
    "    axes[1].set_ylabel(\"Amplitude\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"Converted audio not found. Run inference first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
